{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chatbot using Seq2Seq LSTM models**"
      ],
      "metadata": {
        "id": "O9Ca9uTARbKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is to create conversational chatbot using Sequence to sequence LSTM models. \n",
        "Sequence to sequence learning is about training models to convert from one domain to sequences another domain."
      ],
      "metadata": {
        "id": "URA0bbPhRuaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import all the packages "
      ],
      "metadata": {
        "id": "Gkqdo1vVR-rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n0UKnSGjRGNp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rDThRRtQy1d"
      },
      "source": [
        "## Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "datapath = '/content/drive/MyDrive/Chatbot/clean_conversation.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6l5683rSozN",
        "outputId": "87cf6ed5-8532-4b7b-c201-789b2a05fa0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YT3xecgZu9cy"
      },
      "outputs": [],
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "with open(datapath, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(600, len(lines) - 1)]:\n",
        "    input_text = line.split('\\t')[0]\n",
        "    target_text = line.split('\\t')[1]\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3BTZCofvDDM",
        "outputId": "6aa65371-9532-41f4-cfd3-a0aab7459813"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(input_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Muk4ILXQy1p",
        "outputId": "81cc6118-954b-4ae0-b364-1ef06caefcaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y_gZi8kAujGz"
      },
      "outputs": [],
      "source": [
        "zippedList =  list(zip(input_texts, target_texts))\n",
        "lines = pd.DataFrame(zippedList, columns = ['input' , 'output']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zaot_rwEvXBc",
        "outputId": "6b48cc10-1b68-4def-eb41-c896d28e4c68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             input  \\\n",
              "0          What are your interests   \n",
              "1  What are your favorite subjects   \n",
              "2          What are your interests   \n",
              "3              What is your number   \n",
              "4              What is your number   \n",
              "\n",
              "                                              output  \n",
              "0  I am interested in all kinds of things. We can...  \n",
              "1  My favorite subjects include robotics, compute...  \n",
              "2  I am interested in a wide variety of topics, a...  \n",
              "3                            I don't have any number  \n",
              "4                                        23 skiddoo!  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9312387e-2c7d-4d11-b9bb-74bd40023724\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are your interests</td>\n",
              "      <td>I am interested in all kinds of things. We can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are your favorite subjects</td>\n",
              "      <td>My favorite subjects include robotics, compute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are your interests</td>\n",
              "      <td>I am interested in a wide variety of topics, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is your number</td>\n",
              "      <td>I don't have any number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is your number</td>\n",
              "      <td>23 skiddoo!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9312387e-2c7d-4d11-b9bb-74bd40023724')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9312387e-2c7d-4d11-b9bb-74bd40023724 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9312387e-2c7d-4d11-b9bb-74bd40023724');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "lines.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivp0w4BlQy1v"
      },
      "source": [
        "# Step 3: Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB-LfSHe4-ol",
        "outputId": "1e2df1df-a701-4e98-edd1-865530b5b832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input max length is 22\n",
            "Encoder input data shape -> (572, 22)\n",
            "Number of Input tokens = 552\n"
          ]
        }
      ],
      "source": [
        "input_lines = list()\n",
        "for line in lines.input:\n",
        "    input_lines.append( line ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( input_lines ) \n",
        "tokenized_input_lines = tokenizer.texts_to_sequences( input_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_input_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'Input max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_input_lines = preprocessing.sequence.pad_sequences( tokenized_input_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_input_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "input_word_dict = tokenizer.word_index\n",
        "num_input_tokens = len( input_word_dict )+1\n",
        "print( 'Number of Input tokens = {}'.format( num_input_tokens))\n",
        "np.save('input_word_dict',input_word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUoFquaeQy10"
      },
      "source": [
        "## Preparing input data for the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49gbwsHS4-oy",
        "outputId": "00973059-78f1-4648-924b-8d5dd0d7239c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output max length is 74\n",
            "Decoder input data shape -> (572, 74)\n",
            "Number of Output tokens = 1717\n"
          ]
        }
      ],
      "source": [
        "output_lines = list()\n",
        "for line in lines.output:\n",
        "    output_lines.append( '<START> ' + line + ' <END>' )  \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( output_lines ) \n",
        "tokenized_output_lines = tokenizer.texts_to_sequences( output_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Output max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( tokenized_output_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_output_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "output_word_dict = tokenizer.word_index\n",
        "num_output_tokens = len( output_word_dict )+1\n",
        "print( 'Number of Output tokens = {}'.format( num_output_tokens))\n",
        "np.save('output_word_dict',output_word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K07lywEBQy13"
      },
      "source": [
        "## Preparing target data for the Decoder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXAhyQxL4-o8",
        "outputId": "2589f8fe-628e-4cc7-d75d-d6d7a54b34cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder target data shape -> (572, 74, 1717)\n"
          ]
        }
      ],
      "source": [
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_output_lines = utils.to_categorical( padded_output_lines , num_output_tokens )\n",
        "decoder_target_data = np.array( onehot_output_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--BEEmLQy16"
      },
      "source": [
        "# Step 4: Defining Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkRUlgU-4-pS",
        "outputId": "861d3acf-a47c-42a5-e211-6408fbd0c036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    141312      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    439552      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['embedding_1[0][0]',            \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 1717)   441269      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,072,757\n",
            "Trainable params: 2,072,757\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_input_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_output_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_output_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk7RwNF7Qy19"
      },
      "source": [
        "# Step 5: Training the Model\n",
        "\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical_crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5mbCw1Q4-pd",
        "outputId": "70eb3911-8062-489a-fc9f-faf38aa4b9ae",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/350\n",
            "5/5 [==============================] - 10s 502ms/step - loss: 1.4536 - accuracy: 0.0726\n",
            "Epoch 2/350\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 1.4449 - accuracy: 0.1464\n",
            "Epoch 3/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 1.3651 - accuracy: 0.0992\n",
            "Epoch 4/350\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 1.2025 - accuracy: 0.0702\n",
            "Epoch 5/350\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 1.1511 - accuracy: 0.0696\n",
            "Epoch 6/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 1.1328 - accuracy: 0.0696\n",
            "Epoch 7/350\n",
            "5/5 [==============================] - 3s 494ms/step - loss: 1.1190 - accuracy: 0.0783\n",
            "Epoch 8/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 1.1133 - accuracy: 0.0940\n",
            "Epoch 9/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 1.1066 - accuracy: 0.1035\n",
            "Epoch 10/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 1.1007 - accuracy: 0.1085\n",
            "Epoch 11/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 1.0947 - accuracy: 0.0865\n",
            "Epoch 12/350\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 1.0875 - accuracy: 0.1292\n",
            "Epoch 13/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 1.0797 - accuracy: 0.1505\n",
            "Epoch 14/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 1.0740 - accuracy: 0.1496\n",
            "Epoch 15/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 1.0627 - accuracy: 0.1490\n",
            "Epoch 16/350\n",
            "5/5 [==============================] - 2s 502ms/step - loss: 1.0542 - accuracy: 0.1507\n",
            "Epoch 17/350\n",
            "5/5 [==============================] - 3s 588ms/step - loss: 1.0462 - accuracy: 0.1512\n",
            "Epoch 18/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 1.0390 - accuracy: 0.1505\n",
            "Epoch 19/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 1.0324 - accuracy: 0.1504\n",
            "Epoch 20/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 1.0260 - accuracy: 0.1517\n",
            "Epoch 21/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 1.0192 - accuracy: 0.1546\n",
            "Epoch 22/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 1.0126 - accuracy: 0.1522\n",
            "Epoch 23/350\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 1.0057 - accuracy: 0.1530\n",
            "Epoch 24/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.9986 - accuracy: 0.1565\n",
            "Epoch 25/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.9914 - accuracy: 0.1606\n",
            "Epoch 26/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.9838 - accuracy: 0.1643\n",
            "Epoch 27/350\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.9766 - accuracy: 0.1695\n",
            "Epoch 28/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.9687 - accuracy: 0.1738\n",
            "Epoch 29/350\n",
            "5/5 [==============================] - 2s 502ms/step - loss: 0.9608 - accuracy: 0.1801\n",
            "Epoch 30/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.9528 - accuracy: 0.1882\n",
            "Epoch 31/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.9460 - accuracy: 0.1881\n",
            "Epoch 32/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.9377 - accuracy: 0.1917\n",
            "Epoch 33/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.9301 - accuracy: 0.1957\n",
            "Epoch 34/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.9223 - accuracy: 0.2002\n",
            "Epoch 35/350\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.9147 - accuracy: 0.2095\n",
            "Epoch 36/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.9070 - accuracy: 0.2136\n",
            "Epoch 37/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.8994 - accuracy: 0.2191\n",
            "Epoch 38/350\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.8916 - accuracy: 0.2278\n",
            "Epoch 39/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.8843 - accuracy: 0.2373\n",
            "Epoch 40/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.8762 - accuracy: 0.2424\n",
            "Epoch 41/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.8687 - accuracy: 0.2488\n",
            "Epoch 42/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.8608 - accuracy: 0.2557\n",
            "Epoch 43/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.8532 - accuracy: 0.2606\n",
            "Epoch 44/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.8464 - accuracy: 0.2638\n",
            "Epoch 45/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.8380 - accuracy: 0.2697\n",
            "Epoch 46/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.8308 - accuracy: 0.2740\n",
            "Epoch 47/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.8234 - accuracy: 0.2775\n",
            "Epoch 48/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.8164 - accuracy: 0.2833\n",
            "Epoch 49/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.8090 - accuracy: 0.2840\n",
            "Epoch 50/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.8015 - accuracy: 0.2893\n",
            "Epoch 51/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.7944 - accuracy: 0.2926\n",
            "Epoch 52/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.7876 - accuracy: 0.2948\n",
            "Epoch 53/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.7807 - accuracy: 0.2991\n",
            "Epoch 54/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.7736 - accuracy: 0.3005\n",
            "Epoch 55/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.7669 - accuracy: 0.3030\n",
            "Epoch 56/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.7603 - accuracy: 0.3070\n",
            "Epoch 57/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.7538 - accuracy: 0.3105\n",
            "Epoch 58/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.7471 - accuracy: 0.3124\n",
            "Epoch 59/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.7408 - accuracy: 0.3150\n",
            "Epoch 60/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.7347 - accuracy: 0.3138\n",
            "Epoch 61/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.7281 - accuracy: 0.3189\n",
            "Epoch 62/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.7219 - accuracy: 0.3206\n",
            "Epoch 63/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.7163 - accuracy: 0.3219\n",
            "Epoch 64/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.7097 - accuracy: 0.3258\n",
            "Epoch 65/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.7038 - accuracy: 0.3284\n",
            "Epoch 66/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.6981 - accuracy: 0.3281\n",
            "Epoch 67/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.6914 - accuracy: 0.3358\n",
            "Epoch 68/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.6854 - accuracy: 0.3364\n",
            "Epoch 69/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.6797 - accuracy: 0.3392\n",
            "Epoch 70/350\n",
            "5/5 [==============================] - 2s 466ms/step - loss: 0.6749 - accuracy: 0.3415\n",
            "Epoch 71/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.6678 - accuracy: 0.3457\n",
            "Epoch 72/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.6623 - accuracy: 0.3495\n",
            "Epoch 73/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.6566 - accuracy: 0.3519\n",
            "Epoch 74/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.6509 - accuracy: 0.3571\n",
            "Epoch 75/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.6455 - accuracy: 0.3584\n",
            "Epoch 76/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.6396 - accuracy: 0.3617\n",
            "Epoch 77/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.6341 - accuracy: 0.3639\n",
            "Epoch 78/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.6285 - accuracy: 0.3676\n",
            "Epoch 79/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.6229 - accuracy: 0.3721\n",
            "Epoch 80/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.6174 - accuracy: 0.3741\n",
            "Epoch 81/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.6124 - accuracy: 0.3747\n",
            "Epoch 82/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.6073 - accuracy: 0.3813\n",
            "Epoch 83/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.6012 - accuracy: 0.3809\n",
            "Epoch 84/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.5961 - accuracy: 0.3868\n",
            "Epoch 85/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.5907 - accuracy: 0.3907\n",
            "Epoch 86/350\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.5854 - accuracy: 0.3922\n",
            "Epoch 87/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.5797 - accuracy: 0.3968\n",
            "Epoch 88/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.5742 - accuracy: 0.3982\n",
            "Epoch 89/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.5692 - accuracy: 0.4024\n",
            "Epoch 90/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.5636 - accuracy: 0.4068\n",
            "Epoch 91/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.5586 - accuracy: 0.4105\n",
            "Epoch 92/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.5535 - accuracy: 0.4138\n",
            "Epoch 93/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.5481 - accuracy: 0.4180\n",
            "Epoch 94/350\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.5432 - accuracy: 0.4184\n",
            "Epoch 95/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.5376 - accuracy: 0.4224\n",
            "Epoch 96/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.5331 - accuracy: 0.4277\n",
            "Epoch 97/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.5278 - accuracy: 0.4288\n",
            "Epoch 98/350\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.5222 - accuracy: 0.4362\n",
            "Epoch 99/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.5177 - accuracy: 0.4353\n",
            "Epoch 100/350\n",
            "5/5 [==============================] - 2s 471ms/step - loss: 0.5119 - accuracy: 0.4412\n",
            "Epoch 101/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.5070 - accuracy: 0.4449\n",
            "Epoch 102/350\n",
            "5/5 [==============================] - 2s 471ms/step - loss: 0.5015 - accuracy: 0.4500\n",
            "Epoch 103/350\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.4972 - accuracy: 0.4519\n",
            "Epoch 104/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.4919 - accuracy: 0.4572\n",
            "Epoch 105/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.4867 - accuracy: 0.4639\n",
            "Epoch 106/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.4830 - accuracy: 0.4695\n",
            "Epoch 107/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.4775 - accuracy: 0.4737\n",
            "Epoch 108/350\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.4728 - accuracy: 0.4795\n",
            "Epoch 109/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.4683 - accuracy: 0.4828\n",
            "Epoch 110/350\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.4629 - accuracy: 0.4866\n",
            "Epoch 111/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.4581 - accuracy: 0.4929\n",
            "Epoch 112/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.4530 - accuracy: 0.4963\n",
            "Epoch 113/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.4485 - accuracy: 0.5031\n",
            "Epoch 114/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.4437 - accuracy: 0.5091\n",
            "Epoch 115/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.4395 - accuracy: 0.5124\n",
            "Epoch 116/350\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.4336 - accuracy: 0.5184\n",
            "Epoch 117/350\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.4303 - accuracy: 0.5252\n",
            "Epoch 118/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.4251 - accuracy: 0.5295\n",
            "Epoch 119/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.4207 - accuracy: 0.5319\n",
            "Epoch 120/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.4167 - accuracy: 0.5365\n",
            "Epoch 121/350\n",
            "5/5 [==============================] - 2s 465ms/step - loss: 0.4123 - accuracy: 0.5429\n",
            "Epoch 122/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.4075 - accuracy: 0.5504\n",
            "Epoch 123/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.4037 - accuracy: 0.5489\n",
            "Epoch 124/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.3986 - accuracy: 0.5576\n",
            "Epoch 125/350\n",
            "5/5 [==============================] - 2s 471ms/step - loss: 0.3948 - accuracy: 0.5638\n",
            "Epoch 126/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.3903 - accuracy: 0.5653\n",
            "Epoch 127/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.3857 - accuracy: 0.5713\n",
            "Epoch 128/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.3818 - accuracy: 0.5753\n",
            "Epoch 129/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.3776 - accuracy: 0.5804\n",
            "Epoch 130/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.3736 - accuracy: 0.5855\n",
            "Epoch 131/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.3695 - accuracy: 0.5903\n",
            "Epoch 132/350\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.3656 - accuracy: 0.5904\n",
            "Epoch 133/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.3617 - accuracy: 0.6013\n",
            "Epoch 134/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.3571 - accuracy: 0.6002\n",
            "Epoch 135/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.3536 - accuracy: 0.6070\n",
            "Epoch 136/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.3499 - accuracy: 0.6144\n",
            "Epoch 137/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.3462 - accuracy: 0.6192\n",
            "Epoch 138/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.3426 - accuracy: 0.6242\n",
            "Epoch 139/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.3389 - accuracy: 0.6226\n",
            "Epoch 140/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.3352 - accuracy: 0.6306\n",
            "Epoch 141/350\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.3312 - accuracy: 0.6352\n",
            "Epoch 142/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.3282 - accuracy: 0.6355\n",
            "Epoch 143/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.3236 - accuracy: 0.6447\n",
            "Epoch 144/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.3213 - accuracy: 0.6444\n",
            "Epoch 145/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.3177 - accuracy: 0.6540\n",
            "Epoch 146/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.3143 - accuracy: 0.6543\n",
            "Epoch 147/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.3103 - accuracy: 0.6588\n",
            "Epoch 148/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.3075 - accuracy: 0.6586\n",
            "Epoch 149/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.3031 - accuracy: 0.6675\n",
            "Epoch 150/350\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.3004 - accuracy: 0.6689\n",
            "Epoch 151/350\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.2974 - accuracy: 0.6751\n",
            "Epoch 152/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.2937 - accuracy: 0.6729\n",
            "Epoch 153/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.2915 - accuracy: 0.6855\n",
            "Epoch 154/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.2876 - accuracy: 0.6859\n",
            "Epoch 155/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.2855 - accuracy: 0.6853\n",
            "Epoch 156/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.2817 - accuracy: 0.6883\n",
            "Epoch 157/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.2785 - accuracy: 0.6986\n",
            "Epoch 158/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.2758 - accuracy: 0.7013\n",
            "Epoch 159/350\n",
            "5/5 [==============================] - 2s 468ms/step - loss: 0.2730 - accuracy: 0.7047\n",
            "Epoch 160/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.2699 - accuracy: 0.7064\n",
            "Epoch 161/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.2674 - accuracy: 0.7073\n",
            "Epoch 162/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.2647 - accuracy: 0.7116\n",
            "Epoch 163/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.2611 - accuracy: 0.7151\n",
            "Epoch 164/350\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.2586 - accuracy: 0.7188\n",
            "Epoch 165/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.2565 - accuracy: 0.7192\n",
            "Epoch 166/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.2537 - accuracy: 0.7223\n",
            "Epoch 167/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.2511 - accuracy: 0.7277\n",
            "Epoch 168/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.2482 - accuracy: 0.7284\n",
            "Epoch 169/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.2458 - accuracy: 0.7326\n",
            "Epoch 170/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.2429 - accuracy: 0.7336\n",
            "Epoch 171/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.2405 - accuracy: 0.7399\n",
            "Epoch 172/350\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.2381 - accuracy: 0.7413\n",
            "Epoch 173/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.2353 - accuracy: 0.7450\n",
            "Epoch 174/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.2332 - accuracy: 0.7503\n",
            "Epoch 175/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.2313 - accuracy: 0.7465\n",
            "Epoch 176/350\n",
            "5/5 [==============================] - 2s 502ms/step - loss: 0.2289 - accuracy: 0.7558\n",
            "Epoch 177/350\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.2259 - accuracy: 0.7548\n",
            "Epoch 178/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.2241 - accuracy: 0.7582\n",
            "Epoch 179/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.2221 - accuracy: 0.7627\n",
            "Epoch 180/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.2197 - accuracy: 0.7640\n",
            "Epoch 181/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.2177 - accuracy: 0.7667\n",
            "Epoch 182/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.2156 - accuracy: 0.7652\n",
            "Epoch 183/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.2130 - accuracy: 0.7731\n",
            "Epoch 184/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.2106 - accuracy: 0.7767\n",
            "Epoch 185/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.2088 - accuracy: 0.7719\n",
            "Epoch 186/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.2065 - accuracy: 0.7815\n",
            "Epoch 187/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.2042 - accuracy: 0.7812\n",
            "Epoch 188/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.2026 - accuracy: 0.7847\n",
            "Epoch 189/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.2006 - accuracy: 0.7901\n",
            "Epoch 190/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.1984 - accuracy: 0.7877\n",
            "Epoch 191/350\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.1965 - accuracy: 0.7917\n",
            "Epoch 192/350\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.1945 - accuracy: 0.7917\n",
            "Epoch 193/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1918 - accuracy: 0.8015\n",
            "Epoch 194/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.1911 - accuracy: 0.7984\n",
            "Epoch 195/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.1887 - accuracy: 0.8037\n",
            "Epoch 196/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.1866 - accuracy: 0.8016\n",
            "Epoch 197/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1858 - accuracy: 0.8060\n",
            "Epoch 198/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1837 - accuracy: 0.8056\n",
            "Epoch 199/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.1817 - accuracy: 0.8083\n",
            "Epoch 200/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.1795 - accuracy: 0.8119\n",
            "Epoch 201/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.1786 - accuracy: 0.8149\n",
            "Epoch 202/350\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.1766 - accuracy: 0.8175\n",
            "Epoch 203/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.1747 - accuracy: 0.8155\n",
            "Epoch 204/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.1733 - accuracy: 0.8206\n",
            "Epoch 205/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.1712 - accuracy: 0.8232\n",
            "Epoch 206/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.1689 - accuracy: 0.8239\n",
            "Epoch 207/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.1677 - accuracy: 0.8262\n",
            "Epoch 208/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1665 - accuracy: 0.8262\n",
            "Epoch 209/350\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.1641 - accuracy: 0.8288\n",
            "Epoch 210/350\n",
            "5/5 [==============================] - 2s 462ms/step - loss: 0.1630 - accuracy: 0.8338\n",
            "Epoch 211/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.1611 - accuracy: 0.8379\n",
            "Epoch 212/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1600 - accuracy: 0.8337\n",
            "Epoch 213/350\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.1576 - accuracy: 0.8373\n",
            "Epoch 214/350\n",
            "5/5 [==============================] - 2s 467ms/step - loss: 0.1561 - accuracy: 0.8434\n",
            "Epoch 215/350\n",
            "5/5 [==============================] - 2s 468ms/step - loss: 0.1561 - accuracy: 0.8413\n",
            "Epoch 216/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1544 - accuracy: 0.8380\n",
            "Epoch 217/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.1523 - accuracy: 0.8471\n",
            "Epoch 218/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.1506 - accuracy: 0.8473\n",
            "Epoch 219/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1501 - accuracy: 0.8481\n",
            "Epoch 220/350\n",
            "5/5 [==============================] - 2s 465ms/step - loss: 0.1485 - accuracy: 0.8530\n",
            "Epoch 221/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.1468 - accuracy: 0.8546\n",
            "Epoch 222/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.1448 - accuracy: 0.8551\n",
            "Epoch 223/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1442 - accuracy: 0.8562\n",
            "Epoch 224/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.1424 - accuracy: 0.8571\n",
            "Epoch 225/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1418 - accuracy: 0.8581\n",
            "Epoch 226/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.1395 - accuracy: 0.8631\n",
            "Epoch 227/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.1387 - accuracy: 0.8608\n",
            "Epoch 228/350\n",
            "5/5 [==============================] - 2s 468ms/step - loss: 0.1378 - accuracy: 0.8651\n",
            "Epoch 229/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1363 - accuracy: 0.8639\n",
            "Epoch 230/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.1353 - accuracy: 0.8674\n",
            "Epoch 231/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.1329 - accuracy: 0.8667\n",
            "Epoch 232/350\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.1320 - accuracy: 0.8696\n",
            "Epoch 233/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.1316 - accuracy: 0.8719\n",
            "Epoch 234/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.1301 - accuracy: 0.8747\n",
            "Epoch 235/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.1287 - accuracy: 0.8740\n",
            "Epoch 236/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.1277 - accuracy: 0.8750\n",
            "Epoch 237/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.1271 - accuracy: 0.8746\n",
            "Epoch 238/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.1243 - accuracy: 0.8793\n",
            "Epoch 239/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.1241 - accuracy: 0.8790\n",
            "Epoch 240/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.1235 - accuracy: 0.8783\n",
            "Epoch 241/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.1214 - accuracy: 0.8850\n",
            "Epoch 242/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.1210 - accuracy: 0.8819\n",
            "Epoch 243/350\n",
            "5/5 [==============================] - 2s 468ms/step - loss: 0.1203 - accuracy: 0.8824\n",
            "Epoch 244/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.1192 - accuracy: 0.8846\n",
            "Epoch 245/350\n",
            "5/5 [==============================] - 2s 505ms/step - loss: 0.1176 - accuracy: 0.8864\n",
            "Epoch 246/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.1167 - accuracy: 0.8876\n",
            "Epoch 247/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.1155 - accuracy: 0.8908\n",
            "Epoch 248/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.1145 - accuracy: 0.8887\n",
            "Epoch 249/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.1134 - accuracy: 0.8900\n",
            "Epoch 250/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.1126 - accuracy: 0.8917\n",
            "Epoch 251/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.1111 - accuracy: 0.8950\n",
            "Epoch 252/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.1108 - accuracy: 0.8928\n",
            "Epoch 253/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.1096 - accuracy: 0.8945\n",
            "Epoch 254/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.1081 - accuracy: 0.8963\n",
            "Epoch 255/350\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.1074 - accuracy: 0.8983\n",
            "Epoch 256/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.1064 - accuracy: 0.9003\n",
            "Epoch 257/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.1057 - accuracy: 0.8989\n",
            "Epoch 258/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1047 - accuracy: 0.9047\n",
            "Epoch 259/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.1042 - accuracy: 0.9029\n",
            "Epoch 260/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.1030 - accuracy: 0.9065\n",
            "Epoch 261/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.1022 - accuracy: 0.9066\n",
            "Epoch 262/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.1016 - accuracy: 0.9037\n",
            "Epoch 263/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.0999 - accuracy: 0.9059\n",
            "Epoch 264/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.0995 - accuracy: 0.9059\n",
            "Epoch 265/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.0984 - accuracy: 0.9070\n",
            "Epoch 266/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.0982 - accuracy: 0.9088\n",
            "Epoch 267/350\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.0967 - accuracy: 0.9141\n",
            "Epoch 268/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.0961 - accuracy: 0.9104\n",
            "Epoch 269/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0954 - accuracy: 0.9105\n",
            "Epoch 270/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.0945 - accuracy: 0.9104\n",
            "Epoch 271/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.0942 - accuracy: 0.9105\n",
            "Epoch 272/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.0924 - accuracy: 0.9148\n",
            "Epoch 273/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.0918 - accuracy: 0.9158\n",
            "Epoch 274/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0913 - accuracy: 0.9168\n",
            "Epoch 275/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.0899 - accuracy: 0.9169\n",
            "Epoch 276/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.0887 - accuracy: 0.9180\n",
            "Epoch 277/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0891 - accuracy: 0.9159\n",
            "Epoch 278/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0880 - accuracy: 0.9187\n",
            "Epoch 279/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.0875 - accuracy: 0.9184\n",
            "Epoch 280/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.0871 - accuracy: 0.9193\n",
            "Epoch 281/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.0870 - accuracy: 0.9193\n",
            "Epoch 282/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.0854 - accuracy: 0.9210\n",
            "Epoch 283/350\n",
            "5/5 [==============================] - 2s 506ms/step - loss: 0.0845 - accuracy: 0.9242\n",
            "Epoch 284/350\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.0847 - accuracy: 0.9217\n",
            "Epoch 285/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.0829 - accuracy: 0.9243\n",
            "Epoch 286/350\n",
            "5/5 [==============================] - 2s 468ms/step - loss: 0.0827 - accuracy: 0.9246\n",
            "Epoch 287/350\n",
            "5/5 [==============================] - 2s 461ms/step - loss: 0.0818 - accuracy: 0.9243\n",
            "Epoch 288/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.0813 - accuracy: 0.9267\n",
            "Epoch 289/350\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.0807 - accuracy: 0.9265\n",
            "Epoch 290/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.0801 - accuracy: 0.9280\n",
            "Epoch 291/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.0792 - accuracy: 0.9278\n",
            "Epoch 292/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.0788 - accuracy: 0.9274\n",
            "Epoch 293/350\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.0786 - accuracy: 0.9286\n",
            "Epoch 294/350\n",
            "5/5 [==============================] - 2s 466ms/step - loss: 0.0775 - accuracy: 0.9298\n",
            "Epoch 295/350\n",
            "5/5 [==============================] - 2s 469ms/step - loss: 0.0775 - accuracy: 0.9277\n",
            "Epoch 296/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.0766 - accuracy: 0.9326\n",
            "Epoch 297/350\n",
            "5/5 [==============================] - 2s 470ms/step - loss: 0.0753 - accuracy: 0.9318\n",
            "Epoch 298/350\n",
            "5/5 [==============================] - 2s 476ms/step - loss: 0.0749 - accuracy: 0.9317\n",
            "Epoch 299/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0742 - accuracy: 0.9329\n",
            "Epoch 300/350\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.0739 - accuracy: 0.9343\n",
            "Epoch 301/350\n",
            "5/5 [==============================] - 2s 471ms/step - loss: 0.0732 - accuracy: 0.9338\n",
            "Epoch 302/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0725 - accuracy: 0.9356\n",
            "Epoch 303/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.0727 - accuracy: 0.9335\n",
            "Epoch 304/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.0721 - accuracy: 0.9336\n",
            "Epoch 305/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.0712 - accuracy: 0.9344\n",
            "Epoch 306/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.0705 - accuracy: 0.9358\n",
            "Epoch 307/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.0698 - accuracy: 0.9373\n",
            "Epoch 308/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0688 - accuracy: 0.9383\n",
            "Epoch 309/350\n",
            "5/5 [==============================] - 2s 479ms/step - loss: 0.0686 - accuracy: 0.9375\n",
            "Epoch 310/350\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.0688 - accuracy: 0.9381\n",
            "Epoch 311/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0686 - accuracy: 0.9398\n",
            "Epoch 312/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0679 - accuracy: 0.9406\n",
            "Epoch 313/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.0672 - accuracy: 0.9402\n",
            "Epoch 314/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0668 - accuracy: 0.9408\n",
            "Epoch 315/350\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0660 - accuracy: 0.9415\n",
            "Epoch 316/350\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.0658 - accuracy: 0.9412\n",
            "Epoch 317/350\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.0648 - accuracy: 0.9440\n",
            "Epoch 318/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.0645 - accuracy: 0.9410\n",
            "Epoch 319/350\n",
            "5/5 [==============================] - 2s 461ms/step - loss: 0.0644 - accuracy: 0.9417\n",
            "Epoch 320/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0636 - accuracy: 0.9436\n",
            "Epoch 321/350\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.0629 - accuracy: 0.9445\n",
            "Epoch 322/350\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0632 - accuracy: 0.9427\n",
            "Epoch 323/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.0627 - accuracy: 0.9461\n",
            "Epoch 324/350\n",
            "5/5 [==============================] - 2s 462ms/step - loss: 0.0619 - accuracy: 0.9452\n",
            "Epoch 325/350\n",
            "5/5 [==============================] - 2s 484ms/step - loss: 0.0619 - accuracy: 0.9433\n",
            "Epoch 326/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0612 - accuracy: 0.9433\n",
            "Epoch 327/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0602 - accuracy: 0.9467\n",
            "Epoch 328/350\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0608 - accuracy: 0.9435\n",
            "Epoch 329/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0596 - accuracy: 0.9479\n",
            "Epoch 330/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.0599 - accuracy: 0.9424\n",
            "Epoch 331/350\n",
            "5/5 [==============================] - 2s 478ms/step - loss: 0.0594 - accuracy: 0.9442\n",
            "Epoch 332/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.0590 - accuracy: 0.9487\n",
            "Epoch 333/350\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0583 - accuracy: 0.9479\n",
            "Epoch 334/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0580 - accuracy: 0.9465\n",
            "Epoch 335/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.0576 - accuracy: 0.9488\n",
            "Epoch 336/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0573 - accuracy: 0.9473\n",
            "Epoch 337/350\n",
            "5/5 [==============================] - 2s 473ms/step - loss: 0.0564 - accuracy: 0.9498\n",
            "Epoch 338/350\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.0559 - accuracy: 0.9505\n",
            "Epoch 339/350\n",
            "5/5 [==============================] - 2s 474ms/step - loss: 0.0564 - accuracy: 0.9494\n",
            "Epoch 340/350\n",
            "5/5 [==============================] - 2s 463ms/step - loss: 0.0554 - accuracy: 0.9490\n",
            "Epoch 341/350\n",
            "5/5 [==============================] - 2s 480ms/step - loss: 0.0553 - accuracy: 0.9493\n",
            "Epoch 342/350\n",
            "5/5 [==============================] - 2s 466ms/step - loss: 0.0546 - accuracy: 0.9514\n",
            "Epoch 343/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0544 - accuracy: 0.9516\n",
            "Epoch 344/350\n",
            "5/5 [==============================] - 2s 481ms/step - loss: 0.0543 - accuracy: 0.9498\n",
            "Epoch 345/350\n",
            "5/5 [==============================] - 2s 463ms/step - loss: 0.0535 - accuracy: 0.9508\n",
            "Epoch 346/350\n",
            "5/5 [==============================] - 2s 471ms/step - loss: 0.0531 - accuracy: 0.9531\n",
            "Epoch 347/350\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.0536 - accuracy: 0.9499\n",
            "Epoch 348/350\n",
            "5/5 [==============================] - 2s 475ms/step - loss: 0.0528 - accuracy: 0.9514\n",
            "Epoch 349/350\n",
            "5/5 [==============================] - 2s 472ms/step - loss: 0.0527 - accuracy: 0.9510\n",
            "Epoch 350/350\n",
            "5/5 [==============================] - 2s 477ms/step - loss: 0.0524 - accuracy: 0.9516\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=124, epochs=350) \n",
        "model.save( 'model.h5' ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pKpW5rbQy1_"
      },
      "source": [
        "# Step 6: Defining Inference Models\n",
        "\n",
        "Encoder Inference Model: Takes questions as input and outputs LSTM states (h and c)\n",
        "\n",
        "Decoder Inference Model: Takes in 2 inputs one are the LSTM states, second are the answer input sequences. it will o/p the answers for questions which fed to the encoder model and it's state values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fe8wYN0Z4-pt"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Talking with the Chatbot\n",
        "\n",
        "define a method str_to_tokens which converts str questions to Integer tokens with padding.\n",
        "\n",
        "1. First, we take a question as input and predict the state values using enc_model.\n",
        "2. We set the state values in the decoder's LSTM.\n",
        "3. Then, we generate a sequence which contains the <start> element.\n",
        "4. We input this sequence in the dec_model.\n",
        "5. We replace the <start> element with the element which was predicted by the dec_model and update the state values.\n",
        "6. We carry out the above steps iteratively till we hit the <end> tag or the maximum answer length.\n"
      ],
      "metadata": {
        "id": "P2qPizVkU6rQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VRZQUhXb4-p6"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( input_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "dtO9QVI67N65",
        "outputId": "f196f1e1-de3c-48f8-8606-5f4f3d007647"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Bot: hello\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e36e8307cb7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'User: '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_word_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "enc_model.save( 'enc_model.h5' ) \n",
        "dec_model.save( 'dec_model.h5' ) \n",
        "model.save( 'model.h5' ) \n",
        "\n",
        "for epoch in range( encoder_input_data.shape[0] ):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'User: ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = output_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in output_word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( \"Bot:\" +decoded_translation.replace(' end', '') )\n",
        "    print()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_cxalVo74-qm",
        "outputId": "2e4e926c-96c7-4653-a3fd-e2525bfca848"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9473b174-4c32-446d-9892-4b401992509e\", \"model.h5\", 24932028)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKhKS2SdQy2D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}